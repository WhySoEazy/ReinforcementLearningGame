{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\triet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\triet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-values, returns, and policy\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))  # Q-value for each state-action pair\n",
    "returns_sum = defaultdict(float)  # Total return for each state-action pair\n",
    "returns_count = defaultdict(float)  # Count of how many times each state-action pair is visited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epsilon = 0.1  # Exploration rate\n",
    "gamma = 1.0    # Discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define epsilon-greedy policy\n",
    "def epsilon_greedy_policy(state, epsilon=0.1):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()  # Explore\n",
    "    else:\n",
    "        return np.argmax(Q[state])  # Exploit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate episode\n",
    "def generate_episode(policy, epsilon):\n",
    "    episode = []\n",
    "    state = env.reset()  # Older gym versions return only state, no info\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = policy(state, epsilon)  # Choose action using ε-greedy policy\n",
    "        next_state, reward, done, info = env.step(action)  # Older gym returns 4 values from step()\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Q-values based on the episode\n",
    "def update_Q(episode, gamma):\n",
    "    G = 0  # Initialize the return\n",
    "    visited_state_action_pairs = set()  # Keep track of visited state-action pairs to avoid duplicates\n",
    "    \n",
    "    # Work backwards through the episode\n",
    "    for state, action, reward in reversed(episode):\n",
    "        G = gamma * G + reward  # Calculate the cumulative return\n",
    "        \n",
    "        if (state, action) not in visited_state_action_pairs:\n",
    "            returns_sum[(state, action)] += G\n",
    "            returns_count[(state, action)] += 1\n",
    "            Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]  # Update Q-value\n",
    "            visited_state_action_pairs.add((state, action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent using Monte Carlo control\n",
    "def train_blackjack_MC(episodes, epsilon=0.1, gamma=1.0):\n",
    "    for i in range(episodes):\n",
    "        # Generate an episode following the ε-greedy policy\n",
    "        episode = generate_episode(epsilon_greedy_policy, epsilon)\n",
    "        \n",
    "        # Update Q-values using the generated episode\n",
    "        update_Q(episode, gamma)\n",
    "        \n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent's performance\n",
    "def test_blackjack_MC(episodes):\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    draws = 0\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()  # Older gym returns only state\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(Q[state])  # Always exploit the learned Q-values\n",
    "            state, reward, done, info = env.step(action)  # Older gym returns 4 values\n",
    "\n",
    "        if reward > 0:\n",
    "            wins += 1\n",
    "        elif reward < 0:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    print(f\"Results after {episodes} episodes: {wins} wins, {losses} losses, {draws} draws\")\n",
    "    print('Win rate:' , (wins / episodes) * 100 , '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\triet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed!\n",
      "Results after 100 episodes: 37 wins, 58 losses, 5 draws\n",
      "Win rate: 37.0 %\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "train_blackjack_MC(5000)  # Train for 500,000 episodes\n",
    "\n",
    "# Test the agent\n",
    "test_blackjack_MC(100)  # Test for 1,000 episodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
